{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Weather\n",
    "\n",
    "Mengyu Jackson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Business Problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade --user --upgrade-strategy=eager sktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pprint\n",
    "import time\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.compose import ColumnTransformer,make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, PassiveAggressiveClassifier, RidgeClassifierCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.impute import MissingIndicator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sktime.transformations.panel.rocket import MiniRocket\n",
    "from sktime.utils.data_processing import from_2d_array_to_nested\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix, f1_score, accuracy_score, classification_report\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV, SequentialFeatureSelector, SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherAUS = pd.read_csv('./data/weatherAUS.csv')\n",
    "df = weatherAUS.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                 0\n",
       "Location             0\n",
       "MinTemp           1485\n",
       "MaxTemp           1261\n",
       "Rainfall          3261\n",
       "Evaporation      62790\n",
       "Sunshine         69835\n",
       "WindGustDir      10326\n",
       "WindGustSpeed    10263\n",
       "WindDir9am       10566\n",
       "WindDir3pm        4228\n",
       "WindSpeed9am      1767\n",
       "WindSpeed3pm      3062\n",
       "Humidity9am       2654\n",
       "Humidity3pm       4507\n",
       "Pressure9am      15065\n",
       "Pressure3pm      15028\n",
       "Cloud9am         55888\n",
       "Cloud3pm         59358\n",
       "Temp9am           1767\n",
       "Temp3pm           3609\n",
       "RainToday         3261\n",
       "RainTomorrow      3267\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherAUS.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n",
       "       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n",
       "       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n",
       "       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n",
       "       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n",
       "       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n",
       "       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n",
       "       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n",
       "       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n",
       "       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherAUS[\"Location\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albury: ['Evaporation', 'Sunshine']\n",
      "BadgerysCreek: ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
      "Cobar: []\n",
      "CoffsHarbour: []\n",
      "Moree: []\n",
      "Newcastle: ['Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'Pressure9am', 'Pressure3pm']\n",
      "NorahHead: ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
      "NorfolkIsland: []\n",
      "Penrith: ['Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm']\n",
      "Richmond: ['Sunshine']\n",
      "Sydney: []\n",
      "SydneyAirport: []\n",
      "WaggaWagga: []\n",
      "Williamtown: []\n",
      "Wollongong: ['Evaporation', 'Sunshine']\n",
      "Canberra: []\n",
      "Tuggeranong: ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
      "MountGinini: ['Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm']\n",
      "Ballarat: ['Evaporation', 'Sunshine']\n",
      "Bendigo: ['Sunshine']\n",
      "Sale: []\n",
      "MelbourneAirport: []\n",
      "Melbourne: []\n",
      "Mildura: []\n",
      "Nhil: ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
      "Portland: []\n",
      "Watsonia: []\n",
      "Dartmoor: ['Cloud9am', 'Cloud3pm']\n",
      "Brisbane: []\n",
      "Cairns: []\n",
      "GoldCoast: ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
      "Townsville: []\n",
      "Adelaide: ['Cloud9am', 'Cloud3pm']\n",
      "MountGambier: []\n",
      "Nuriootpa: []\n",
      "Woomera: []\n",
      "Albany: ['WindGustDir', 'WindGustSpeed']\n",
      "Witchcliffe: ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
      "PearceRAAF: ['Evaporation']\n",
      "PerthAirport: []\n",
      "Perth: []\n",
      "SalmonGums: ['Evaporation', 'Sunshine', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm']\n",
      "Walpole: ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm']\n",
      "Hobart: []\n",
      "Launceston: ['Sunshine']\n",
      "AliceSprings: []\n",
      "Darwin: []\n",
      "Katherine: ['Sunshine']\n",
      "Uluru: ['Evaporation', 'Sunshine']\n",
      "{'Adelaide': ['Cloud9am', 'Cloud3pm'],\n",
      " 'Albany': ['WindGustDir', 'WindGustSpeed'],\n",
      " 'Albury': ['Evaporation', 'Sunshine'],\n",
      " 'AliceSprings': [],\n",
      " 'BadgerysCreek': ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'],\n",
      " 'Ballarat': ['Evaporation', 'Sunshine'],\n",
      " 'Bendigo': ['Sunshine'],\n",
      " 'Brisbane': [],\n",
      " 'Cairns': [],\n",
      " 'Canberra': [],\n",
      " 'Cobar': [],\n",
      " 'CoffsHarbour': [],\n",
      " 'Dartmoor': ['Cloud9am', 'Cloud3pm'],\n",
      " 'Darwin': [],\n",
      " 'GoldCoast': ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'],\n",
      " 'Hobart': [],\n",
      " 'Katherine': ['Sunshine'],\n",
      " 'Launceston': ['Sunshine'],\n",
      " 'Melbourne': [],\n",
      " 'MelbourneAirport': [],\n",
      " 'Mildura': [],\n",
      " 'Moree': [],\n",
      " 'MountGambier': [],\n",
      " 'MountGinini': ['Evaporation',\n",
      "                 'Sunshine',\n",
      "                 'Pressure9am',\n",
      "                 'Pressure3pm',\n",
      "                 'Cloud9am',\n",
      "                 'Cloud3pm'],\n",
      " 'Newcastle': ['Evaporation',\n",
      "               'Sunshine',\n",
      "               'WindGustDir',\n",
      "               'WindGustSpeed',\n",
      "               'Pressure9am',\n",
      "               'Pressure3pm'],\n",
      " 'Nhil': ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'],\n",
      " 'NorahHead': ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'],\n",
      " 'NorfolkIsland': [],\n",
      " 'Nuriootpa': [],\n",
      " 'PearceRAAF': ['Evaporation'],\n",
      " 'Penrith': ['Evaporation',\n",
      "             'Sunshine',\n",
      "             'Pressure9am',\n",
      "             'Pressure3pm',\n",
      "             'Cloud9am',\n",
      "             'Cloud3pm'],\n",
      " 'Perth': [],\n",
      " 'PerthAirport': [],\n",
      " 'Portland': [],\n",
      " 'Richmond': ['Sunshine'],\n",
      " 'Sale': [],\n",
      " 'SalmonGums': ['Evaporation',\n",
      "                'Sunshine',\n",
      "                'Pressure9am',\n",
      "                'Pressure3pm',\n",
      "                'Cloud9am',\n",
      "                'Cloud3pm'],\n",
      " 'Sydney': [],\n",
      " 'SydneyAirport': [],\n",
      " 'Townsville': [],\n",
      " 'Tuggeranong': ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'],\n",
      " 'Uluru': ['Evaporation', 'Sunshine'],\n",
      " 'WaggaWagga': [],\n",
      " 'Walpole': ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'],\n",
      " 'Watsonia': [],\n",
      " 'Williamtown': [],\n",
      " 'Witchcliffe': ['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'],\n",
      " 'Wollongong': ['Evaporation', 'Sunshine'],\n",
      " 'Woomera': []}\n"
     ]
    }
   ],
   "source": [
    "data_by_location = {\n",
    "    loc: weatherAUS[weatherAUS[\"Location\"]==loc].copy()\n",
    "    for loc in weatherAUS[\"Location\"].unique()\n",
    "}\n",
    "\n",
    "na_columns_by_loc = {}\n",
    "for loc, df in data_by_location.items():\n",
    "    na_columns_by_loc[loc] = [\n",
    "        col for col in df.columns\n",
    "        if all(df[col].isna())\n",
    "    ]\n",
    "\n",
    "for loc, na_columns in na_columns_by_loc.items():\n",
    "    df = data_by_location[loc]\n",
    "    print(f\"{loc}: {na_columns}\")\n",
    "    for col in na_columns:\n",
    "\n",
    "        df.pop(col)\n",
    "        \n",
    "pprint.pprint(na_columns_by_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_na_columns = sorted(set(sum(na_columns_by_loc.values(), [])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_column_df = pd.DataFrame([\n",
    "    [1 if col in na_columns_by_loc[loc] else 0 \n",
    "             for col in all_na_columns ]\n",
    "    for loc in na_columns_by_loc.keys()],\n",
    "    index=list(na_columns_by_loc.keys()),\n",
    "    columns=all_na_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143882   2013-03-01\n",
       "143883   2013-03-02\n",
       "143884   2013-03-03\n",
       "143885   2013-03-04\n",
       "143886   2013-03-05\n",
       "            ...    \n",
       "145455   2017-06-21\n",
       "145456   2017-06-22\n",
       "145457   2017-06-23\n",
       "145458   2017-06-24\n",
       "145459   2017-06-25\n",
       "Name: Date, Length: 1578, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df.pop('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-240df369eb52>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['RainToday'] = df['RainToday'].replace('No', 0).replace('Yes', 1).astype(float)\n",
      "<ipython-input-12-240df369eb52>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['RainTomorrow'] = df['RainTomorrow'].replace('No', 0).replace('Yes', 1).astype(float)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['RainToday', 'RainTomorrow'])\n",
    "df['RainToday'] = df['RainToday'].replace('No', 0).replace('Yes', 1).astype(float)\n",
    "df['RainTomorrow'] = df['RainTomorrow'].replace('No', 0).replace('Yes', 1).astype(float)\n",
    "\n",
    "#df['RainTomorrow'].replace('No', 0).replace('Yes', 1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-d4ae8bfb13d4>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['WindGustDir'] = df['WindGustDir'].fillna(\"NaN\")\n",
      "<ipython-input-13-d4ae8bfb13d4>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['WindDir9am'] = df['WindDir9am'].fillna(\"NaN\")\n",
      "<ipython-input-13-d4ae8bfb13d4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['WindDir3pm'] = df['WindDir3pm'].fillna(\"NaN\")\n"
     ]
    }
   ],
   "source": [
    "df['WindGustDir'] = df['WindGustDir'].fillna(\"NaN\")\n",
    "df['WindDir9am'] = df['WindDir9am'].fillna(\"NaN\")\n",
    "df['WindDir3pm'] = df['WindDir3pm'].fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1502"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = df.dropna()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_full = df[df['Year']>=2016].copy()\n",
    "y_test_full = x_test_full.pop('RainTomorrow')\n",
    "x_train_full = df[df['Year']<2016].copy()\n",
    "y_train_full = x_train_full.pop('RainTomorrow')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location          object\n",
       "MinTemp          float64\n",
       "MaxTemp          float64\n",
       "Rainfall         float64\n",
       "WindGustDir       object\n",
       "WindGustSpeed    float64\n",
       "WindDir9am        object\n",
       "WindDir3pm        object\n",
       "WindSpeed9am     float64\n",
       "WindSpeed3pm     float64\n",
       "Humidity9am      float64\n",
       "Humidity3pm      float64\n",
       "Pressure9am      float64\n",
       "Pressure3pm      float64\n",
       "Cloud9am         float64\n",
       "Cloud3pm         float64\n",
       "Temp9am          float64\n",
       "Temp3pm          float64\n",
       "RainToday        float64\n",
       "RainTomorrow     float64\n",
       "Year               int64\n",
       "Month              int64\n",
       "Day                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('minmaxscaler', MinMaxScaler(),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x0000024165B27E50>),\n",
       "                                ('onehotencoder',\n",
       "                                 OneHotEncoder(handle_unknown='ignore'),\n",
       "                                 <sklearn.compose._column_transformer.make_column_selector object at 0x0000024165B27F70>)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column_transformer = ColumnTransformer([\n",
    "#     (\"windgustdir\",OneHotEncoder(), [\"WindGustDir\"]),\n",
    "#     (\"winddir9am\",OneHotEncoder(), [\"WindDir9am\"]),\n",
    "#     (\"winddir3pm\",OneHotEncoder(), [\"WindDir3pm\"]),\n",
    "#     (\"loc\",OneHotEncoder(), [\"Location\"]),], \n",
    "#     remainder=\"passthrough\",\n",
    "# )\n",
    "column_transformer = make_column_transformer(\n",
    "    (MinMaxScaler(),\n",
    "     make_column_selector(dtype_include=np.number)),\n",
    "    (OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "     make_column_selector(dtype_include=object)))\n",
    "\n",
    "column_transformer.fit(x_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>143882</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>19.7</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>ESE</td>\n",
       "      <td>48.0</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.7</td>\n",
       "      <td>28.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143883</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>21.6</td>\n",
       "      <td>33.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>33.0</td>\n",
       "      <td>E</td>\n",
       "      <td>N</td>\n",
       "      <td>22.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.5</td>\n",
       "      <td>1006.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.6</td>\n",
       "      <td>31.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143884</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>21.3</td>\n",
       "      <td>36.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>33.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>SSE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1006.9</td>\n",
       "      <td>1002.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.6</td>\n",
       "      <td>34.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143885</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>22.9</td>\n",
       "      <td>37.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>39.0</td>\n",
       "      <td>E</td>\n",
       "      <td>SSE</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>1002.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.7</td>\n",
       "      <td>35.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143886</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>S</td>\n",
       "      <td>39.0</td>\n",
       "      <td>E</td>\n",
       "      <td>S</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1006.9</td>\n",
       "      <td>1003.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.9</td>\n",
       "      <td>37.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144913</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>20.5</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>52.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>E</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1013.2</td>\n",
       "      <td>1010.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.3</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144914</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>54.0</td>\n",
       "      <td>E</td>\n",
       "      <td>ESE</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1014.7</td>\n",
       "      <td>1010.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.7</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144915</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>17.5</td>\n",
       "      <td>37.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>56.0</td>\n",
       "      <td>E</td>\n",
       "      <td>SE</td>\n",
       "      <td>33.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1012.6</td>\n",
       "      <td>1007.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.1</td>\n",
       "      <td>34.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144916</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>20.0</td>\n",
       "      <td>38.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>E</td>\n",
       "      <td>59.0</td>\n",
       "      <td>E</td>\n",
       "      <td>SSE</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>1002.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.3</td>\n",
       "      <td>38.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144917</th>\n",
       "      <td>Uluru</td>\n",
       "      <td>19.3</td>\n",
       "      <td>37.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>56.0</td>\n",
       "      <td>ESE</td>\n",
       "      <td>S</td>\n",
       "      <td>20.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.3</td>\n",
       "      <td>1003.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.8</td>\n",
       "      <td>35.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Location  MinTemp  MaxTemp  Rainfall WindGustDir  WindGustSpeed  \\\n",
       "143882    Uluru     19.7     30.0       0.8         ESE           48.0   \n",
       "143883    Uluru     21.6     33.1       0.0           E           33.0   \n",
       "143884    Uluru     21.3     36.1       0.0           E           33.0   \n",
       "143885    Uluru     22.9     37.7       0.0         ENE           39.0   \n",
       "143886    Uluru     24.0     39.0       0.0           S           39.0   \n",
       "...         ...      ...      ...       ...         ...            ...   \n",
       "144913    Uluru     20.5     34.7       0.0           E           52.0   \n",
       "144914    Uluru     18.0     36.4       0.0         ESE           54.0   \n",
       "144915    Uluru     17.5     37.1       0.0           E           56.0   \n",
       "144916    Uluru     20.0     38.9       0.0           E           59.0   \n",
       "144917    Uluru     19.3     37.4       0.0          SE           56.0   \n",
       "\n",
       "       WindDir9am WindDir3pm  WindSpeed9am  WindSpeed3pm  ...  Pressure9am  \\\n",
       "143882          E          E          30.0          24.0  ...       1010.6   \n",
       "143883          E          N          22.0          11.0  ...       1010.5   \n",
       "143884        ENE        SSE          24.0          13.0  ...       1006.9   \n",
       "143885          E        SSE          28.0          13.0  ...       1006.0   \n",
       "143886          E          S          20.0          19.0  ...       1006.9   \n",
       "...           ...        ...           ...           ...  ...          ...   \n",
       "144913        ESE          E          35.0          20.0  ...       1013.2   \n",
       "144914          E        ESE          30.0          31.0  ...       1014.7   \n",
       "144915          E         SE          33.0          22.0  ...       1012.6   \n",
       "144916          E        SSE          20.0          17.0  ...       1007.2   \n",
       "144917        ESE          S          20.0          28.0  ...       1007.3   \n",
       "\n",
       "        Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  Year  \\\n",
       "143882       1007.5       NaN       NaN     21.7     28.4        0.0  2013   \n",
       "143883       1006.5       NaN       NaN     24.6     31.3        0.0  2013   \n",
       "143884       1002.7       NaN       NaN     27.6     34.5        0.0  2013   \n",
       "143885       1002.1       NaN       NaN     28.7     35.4        0.0  2013   \n",
       "143886       1003.5       NaN       NaN     29.9     37.3        0.0  2013   \n",
       "...             ...       ...       ...      ...      ...        ...   ...   \n",
       "144913       1010.1       NaN       NaN     24.3     33.0        0.0  2015   \n",
       "144914       1010.9       NaN       NaN     26.7     35.0        0.0  2015   \n",
       "144915       1007.5       NaN       NaN     28.1     34.7        0.0  2015   \n",
       "144916       1002.6       NaN       1.0     31.3     38.4        0.0  2015   \n",
       "144917       1003.7       NaN       NaN     25.8     35.8        0.0  2015   \n",
       "\n",
       "        Month  Day  \n",
       "143882      3    1  \n",
       "143883      3    2  \n",
       "143884      3    3  \n",
       "143885      3    4  \n",
       "143886      3    5  \n",
       "...       ...  ...  \n",
       "144913     12   27  \n",
       "144914     12   28  \n",
       "144915     12   29  \n",
       "144916     12   30  \n",
       "144917     12   31  \n",
       "\n",
       "[975 rows x 22 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# foo = pd.DataFrame(data=column_transformer.transform(x_train).toarray())\n",
    "\n",
    "# column_transformer.transform(x_train)\n",
    "x_train_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Model\n",
    "\n",
    "* RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc = make_pipeline(column_transformer, \n",
    "#                     FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n",
    "#                     MinMaxScaler(),\n",
    "#                     KNNImputer(), \n",
    "#                     RandomForestClassifier(random_state=random_state=None))\n",
    "# rfc.fit(x_train, y_train)\n",
    "# rfc.score(x_test, y_test)\n",
    "\n",
    "\n",
    "# Score is 0.8453908984830805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Uluru'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-af645b63180a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m--> 343\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse=['csr', 'csc'],\n\u001b[0m\u001b[0;32m    344\u001b[0m                                    multi_output=True)\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1898\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1899\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1901\u001b[0m     def __array_wrap__(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Uluru'"
     ]
    }
   ],
   "source": [
    "X_new = SelectKBest(chi2, k=20).fit_transform(x_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = make_pipeline(column_transformer,\n",
    "                    SimpleImputer(), \n",
    "                    RandomForestClassifier(random_state=RANDOM_SEED))\n",
    "\n",
    "trimmed_rfc = SequentialFeatureSelector(estimator=rfc)\n",
    "trimmed_rfc.fit(x_train_full, y_train_full)\n",
    "trimmed_rfc.estimator_.score(x_test_full, y_test_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{x_train_full.columns[trimmed_rfc.get_support()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(rfc.named_steps.randomforestclassifier, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline_preprocessing = make_pipeline(column_transformer, \n",
    "                    MaxAbsScaler(),\n",
    "                    SimpleImputer(),\n",
    "                                  )\n",
    "transform_pipeline_preprocessing.fit(x_train_full)\n",
    "transform_pipeline = make_pipeline(transform_pipeline_preprocessing,\n",
    "                                                                     model)\n",
    "\n",
    "new_columns = transform_pipeline.transform(x_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_features_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.named_steps.randomforestclassifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in data_by_location.values():\n",
    "    print(df.columns)\n",
    "    df.dropna(subset=['RainToday', 'RainTomorrow'], inplace=True)\n",
    "    df['RainToday'] = df['RainToday'].replace('No', 0).replace('Yes', 1).astype(float)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df.set_index(\"Date\", inplace=True)\n",
    "    period_index = df.index.to_period(\"D\")\n",
    "    df.index = df.index.to_period(\"D\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_by_location, X_by_location = {}, {}\n",
    "\n",
    "for loc, df in data_by_location.items():\n",
    "    X_by_location[loc] = df.copy()\n",
    "    y_by_location[loc] = X_by_location[loc].pop(\"RainTomorrow\")\n",
    "    X_by_location[loc].pop(\"Location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_by_loc, X_test_by_loc, y_train_by_loc, y_test_by_loc = {}, {}, {}, {}\n",
    "\n",
    "for loc in data_by_location.keys():\n",
    "    X_train_by_loc[loc], X_test_by_loc[loc], y_train_by_loc[loc], y_test_by_loc[loc] = temporal_train_test_split(X_by_location[loc], y_by_location[loc], train_size=.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_by_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer_by_loc = {}\n",
    "\n",
    "for loc, X_train in X_train_by_loc.items():\n",
    "    encoders = []\n",
    "    for col in [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"]:\n",
    "        if col in X_train.columns:\n",
    "            encoders.append((col.lower(), OneHotEncoder(handle_unknown = 'ignore'), [col]))\n",
    "    \n",
    "    column_transformer_by_loc[loc] = ColumnTransformer(encoders, remainder=\"passthrough\", sparse_threshold=0)\n",
    "    #colun_transformer_by_loc[loc].fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline_by_loc = {\n",
    "    loc: make_pipeline(\n",
    "        transformer,\n",
    "        MaxAbsScaler(),\n",
    "        KNNImputer(),\n",
    "    )\n",
    "    for loc, transformer in column_transformer_by_loc.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed_by_loc = {\n",
    "    loc: from_2d_array_to_nested(transform_pipeline.fit_transform(X_train_by_loc[loc]))\n",
    "    for loc, transform_pipeline in transform_pipeline_by_loc.items()\n",
    "}\n",
    "\n",
    "for loc, X_train_transformed in X_train_transformed_by_loc.items():\n",
    "    X_train_transformed.index = X_train_by_loc[loc].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "inference_pipeline_by_loc = {\n",
    "    loc: make_pipeline(\n",
    "        MiniRocket(), \n",
    "        RidgeClassifierCV(alphas=np.logspace(-3, 3, 10), normalize=True)\n",
    "    )\n",
    "    for loc in X_train_transformed_by_loc.keys()\n",
    "}\n",
    "\n",
    "for loc, inference_pipeline in inference_pipeline_by_loc.items():\n",
    "    inference_pipeline.fit(X_train_transformed_by_loc[loc], y_train_by_loc[loc])\n",
    "    \n",
    "training_time = time.time() - start\n",
    "print(f\"Training took {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time is over 12 hours; vs 202 seconds for the Rocket model.\n",
    "\n",
    "# from sktime.classification.hybrid import HIVECOTEV1\n",
    "# start2 = time.time()\n",
    "\n",
    "# inference_pipeline_by_loc2 = {\n",
    "#     loc: make_pipeline(\n",
    "#         HIVECOTEV1()\n",
    "#     )\n",
    "#     for loc in X_train_transformed_by_loc.keys()\n",
    "# }\n",
    "\n",
    "# for loc, inference_pipeline in inference_pipeline_by_loc2.items():\n",
    "#     inference_pipeline.fit(X_train_transformed_by_loc[loc], y_train_by_loc[loc])\n",
    "    \n",
    "# training_time2 = time.time() - start2\n",
    "# print(f\"Training took {training_time2} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed_by_loc = {\n",
    "    loc: from_2d_array_to_nested(transform_pipeline.transform(X_test_by_loc[loc])) \n",
    "    #loc: transform_pipeline.transform(X_test_by_loc[loc])\n",
    "    for loc, transform_pipeline in transform_pipeline_by_loc.items()\n",
    "}\n",
    "\n",
    "# for loc, transform_pipeline in transform_pipeline_by_loc.items():\n",
    "#     print(loc)\n",
    "#     transform_pipeline.transform(X_test_by_loc[loc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_by_loc = {\n",
    "    loc: inference_pipeline.predict(X_test_transformed_by_loc[loc])\n",
    "    for loc, inference_pipeline in inference_pipeline_by_loc.items()\n",
    "}\n",
    "y_pred_proba_by_loc = {\n",
    "    loc: inference_pipeline.predict_proba(X_test_transformed_by_loc[loc])\n",
    "    for loc, inference_pipeline in inference_pipeline_by_loc.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_by_loc = {loc: len(df) for loc, df in data_by_location.items()}\n",
    "len_by_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_by_loc = {loc: f1_score(y_pred_by_loc[loc], y_test_by_loc[loc], pos_label=\"Yes\") for loc in y_pred_by_loc.keys()}\n",
    "\n",
    "pprint.pprint(f1_score_by_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_no_nas = pd.Series(sorted(score_by_loc[loc]\n",
    "                   for loc, na_columns in na_columns_by_loc.items()\n",
    "                   if not na_columns))\n",
    "f1_scores_no_nas\n",
    "\n",
    "f1_scores_no_nas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores_with_nas = pd.Series(sorted(score_by_loc[loc]\n",
    "                   for loc, na_columns in na_columns_by_loc.items()\n",
    "                   if na_columns))\n",
    "f1_scores_with_nas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_by_loc = {\n",
    "    loc: inference_pipeline.score(X_test_transformed_by_loc[loc], y_test_by_loc[loc])\n",
    "    for loc, inference_pipeline in inference_pipeline_by_loc.items()\n",
    "}\n",
    "\n",
    "confusion_matrix_by_loc = {\n",
    "    loc: confusion_matrix(y_true, y_pred_by_loc[loc])\n",
    "    for loc, y_true in y_test_by_loc.items()\n",
    "}\n",
    "\n",
    "#inference_pipeline[\"Albury\"].score(X_test_transformed_by_loc[\"Albury\"], y_test_by_loc[\"Albury\"])\n",
    "pprint.pprint(score_by_loc)\n",
    "pprint.pprint(confusion_matrix_by_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_column_df_y = pd.Series((f1_score_by_loc[loc] for loc in na_column_df.index), index=na_column_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_X_train, na_X_test, na_y_train, na_y_test = train_test_split(na_column_df, na_column_df_y, test_size=0.33, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_regressor = LinearSVR()\n",
    "na_regressor.fit(na_X_train, na_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_regressor.score(na_X_test, na_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_regressor.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Confusion Matrix: {sum(confusion_matrix_by_loc.values()}\")\n",
    "print(f\"Accuracy: )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_y_true = []\n",
    "# combined_y_pred = []\n",
    "\n",
    "# for loc in y_test_by_loc.keys():\n",
    "\n",
    "y_test_by_loc[\"Albury\"].to_list() + y_test_by_loc[\"Melbourne\"].to_list()\n",
    "combined_y_test = sum((list(v) for v in y_test_by_loc.values()), [])\n",
    "combined_y_pred = sum((list(v) for v in y_pred_by_loc.values()), [])\n",
    "#y_pred_by_loc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {accuracy_score(combined_y_test, combined_y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(combined_y_test, combined_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test_by_loc[\"Woomera\"], y_pred_by_loc[\"Woomera\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc, inference_pipeline in inference_pipeline_by_loc.items():\n",
    "    print(f\"{loc}\")\n",
    "    plot_confusion_matrix(inference_pipeline, X_test_transformed_by_loc[loc], y_test_by_loc[loc]) \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(score_by_loc.values()) / len(score_by_loc) for loc in score_by_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_precision = average_precision_score(x_test_transformed, ts_y_test)\n",
    "\n",
    "# print('Average precision-recall score: {0:0.2f}'.format(\n",
    "#       average_precision))\n",
    "average_precision=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = \"Woomera\"\n",
    "disp = plot_precision_recall_curve(inference_pipeline_by_loc[loc], X_test_transformed_by_loc[loc], y_test_by_loc[loc])\n",
    "disp.ax_.set_title('2-class Precision-Recall curve: '\n",
    "                   'AP={0:0.2f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer([\n",
    "    (\"windgustdir\",OneHotEncoder(handle_unknown = 'ignore'), [\"WindGustDir\"]),\n",
    "    (\"winddir9am\",OneHotEncoder(handle_unknown = 'ignore'), [\"WindDir9am\"]),\n",
    "    (\"winddir3pm\",OneHotEncoder(handle_unknown = 'ignore'), [\"WindDir3pm\"]),\n",
    "    (\"loc\",OneHotEncoder(), [\"Location\"]),], \n",
    "    remainder=\"passthrough\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict = {}\n",
    "\n",
    "for i, classifier in enumerate([\n",
    "    make_pipeline(column_transformer, \n",
    "                  MaxAbsScaler(),\n",
    "                  SimpleImputer(), \n",
    "                  RandomForestClassifier()\n",
    "                 ),\n",
    "    make_pipeline(\n",
    "        column_transformer,\n",
    "        MaxAbsScaler(),\n",
    "        SimpleImputer(), \n",
    "        SelectFromModel(LinearSVC(penalty=\"l1\", dual=False)),\n",
    "        RandomForestClassifier()\n",
    "    )\n",
    "]):\n",
    "    if classifier_dict.get(type(classifier).__name__) is None:\n",
    "        start = time.time()\n",
    "        classifier.fit(x_train_full, y_train_full)\n",
    "        score = classifier.score(x_test_full, y_test_full)\n",
    "        training_time = time.time() - start\n",
    "        c_name = f\"{type(classifier).__name__}_{i}\"\n",
    "        print(f\"{type(classifier).__name__} ({training_time} seconds): {score}\")\n",
    "        classifier_dict[type(classifier).__name__] = {\"classifier\": pipeline, \"score\": score, \"training_time\": training_time}\n",
    "    else:\n",
    "        pipeline = classifier_dict[type(classifier).__name__]\n",
    "    plot_confusion_matrix(classifier, x_test, y_test) \n",
    "\n",
    "pprint.pprint(classifier_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict = {}\n",
    "\n",
    "for classifier in [\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(),\n",
    "    Perceptron(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    #NuSVC(nu=0.2),\n",
    "    LinearSVC(),\n",
    "    #SVC(), Too slow (600+ seconds) and outperformed by LinearSVC anyways\n",
    "    KNeighborsClassifier(),\n",
    "    #RadiusNeighborsClassifier(radius=1.5), # Nothing found in radius, even after increase\n",
    "    NearestCentroid(),\n",
    "    #MultinomialNB(), Negative values?\n",
    "    BernoulliNB(),\n",
    "    DecisionTreeClassifier(),\n",
    "    BaggingClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    #ExtraTreesClassifier(), Too slow, (400+ seconds) and no meaningful difference from Bagging/RandomForest\n",
    "    AdaBoostClassifier(),\n",
    "    MLPClassifier(),\n",
    "]:\n",
    "    if classifier_dict.get(type(classifier).__name__) is None:\n",
    "        start = time.time()\n",
    "        pipeline = make_pipeline(column_transformer, \n",
    "                        MaxAbsScaler(),\n",
    "                        SimpleImputer(), \n",
    "                        classifier)\n",
    "        pipeline.fit(x_train, y_train)\n",
    "        score = pipeline.score(x_test, y_test)\n",
    "        training_time = time.time() - start\n",
    "        c_name = f\"{type(classifier).__name__}_{i}\"\n",
    "        print(f\"{type(classifier).__name__} ({training_time} seconds): {score}\")\n",
    "        classifier_dict[type(classifier).__name__] = {\"classifier\": pipeline, \"score\": score, \"training_time\": training_time}\n",
    "    else:\n",
    "        pipeline = classifier_dict[type(classifier).__name__]\n",
    "    plot_confusion_matrix(pipeline, x_test, y_test) \n",
    "\n",
    "pprint.pprint(classifier_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict = {}\n",
    "\n",
    "for i, classifier in enumerate([\n",
    "    MLPClassifier(learning_rate=\"adaptive\", learning_rate_init=0.01, max_iter=1000, random_state=0),\n",
    "    MLPClassifier(hidden_layer_sizes=(100,100),  learning_rate=\"adaptive\", learning_rate_init=0.01, max_iter=1000, random_state=0),\n",
    "    MLPClassifier(hidden_layer_sizes=(50,50),  learning_rate=\"adaptive\", learning_rate_init=0.01, max_iter=1000, random_state=0),\n",
    "    MLPClassifier(hidden_layer_sizes=(30,30,30),  learning_rate=\"adaptive\", learning_rate_init=0.01, max_iter=1000, random_state=0)\n",
    "]):\n",
    "    start = time.time()\n",
    "    pipeline = make_pipeline(column_transformer, \n",
    "                    MaxAbsScaler(),\n",
    "                    SimpleImputer(), \n",
    "                    classifier)\n",
    "    pipeline.fit(x_train, y_train)\n",
    "    score = pipeline.score(x_test, y_test)\n",
    "    training_time = time.time() - start\n",
    "    c_name = f\"{type(classifier).__name__}_{i}\"\n",
    "    print(f\"{c_name} ({training_time} seconds): {score}\")\n",
    "    classifier_dict[c_name] = {\"classifier\": pipeline, \"score\": score, \"training_time\": training_time}\n",
    "    plot_confusion_matrix(pipeline, x_test, y_test) \n",
    "\n",
    "pprint.pprint(classifier_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_dict = {}\n",
    "\n",
    "for i, classifier in enumerate([\n",
    "    RandomForestClassifier(n_estimators=1000, min_samples_leaf=100, oob_score=True, class_weight=\"balanced\"),\n",
    "]):\n",
    "    if classifier_dict.get(type(classifier).__name__) is None:\n",
    "        start = time.time()\n",
    "        pipeline = make_pipeline(column_transformer, \n",
    "                        MaxAbsScaler(),\n",
    "                        SimpleImputer(), \n",
    "                        classifier)\n",
    "        pipeline.fit(x_train, y_train)\n",
    "        score = pipeline.score(x_test, y_test)\n",
    "        training_time = time.time() - start\n",
    "        c_name = f\"{type(classifier).__name__}_{i}\"\n",
    "        print(f\"{type(classifier).__name__} ({training_time} seconds): {score}\")\n",
    "        classifier_dict[type(classifier).__name__] = {\"classifier\": pipeline, \"score\": score, \"training_time\": training_time}\n",
    "    else:\n",
    "        pipeline = classifier_dict[type(classifier).__name__]\n",
    "    plot_confusion_matrix(pipeline, x_test, y_test) \n",
    "\n",
    "pprint.pprint(classifier_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = plot_precision_recall_curve(classifier_dict[\"RandomForestClassifier\"][\"classifier\"], x_test, y_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc = make_pipeline(column_transformer, LogisticRegression(random_state=0))\n",
    "lrc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drc = make_pipeline(column_transformer, DecisionTreeClassifier(random_state=0))\n",
    "drc.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model\n",
    "\n",
    "## Linear Model Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
